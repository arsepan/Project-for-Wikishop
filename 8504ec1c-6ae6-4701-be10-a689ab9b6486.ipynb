{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70959705",
   "metadata": {},
   "source": [
    "## Проект для \"Викишоп\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb120ed5",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В моём распоряжении набор данных с разметкой о токсичности правок. Задача: **построить модель со значением метрики качества F1 не меньше 0.75**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef21d2",
   "metadata": {},
   "source": [
    "## Импорт, функции, константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0138481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Испортируем необходимые библиотеки\n",
    "\n",
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "import math\n",
    "\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "\n",
    "#!pip install transformers\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaccc158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn                      0.24.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep scikit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe8229",
   "metadata": {},
   "source": [
    "## Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f18c6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Распакуем наши данные\n",
    "try:\n",
    "    data = pd.read_csv('toxic_comments.csv', index_col = 0)\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', index_col = 0)\n",
    "display(data.head(3))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6371294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.898388\n",
       "1    0.101612\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим дисбаланс классов\n",
    "data['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b1801",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a186185",
   "metadata": {},
   "source": [
    "Для обучения модели нам был предоставлен DF с 159.292 наблюдениями. NaN значений нет, столбцы названы коррестно, единственная проблема - дисбаланс, её пофиксим на этапе прогноза или подготовки признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034aacb",
   "metadata": {},
   "source": [
    "## Подготовка признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee845be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their foot for good'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "\" \".join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f1a16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208613148f644f3398bcaff01f4d2e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour I be see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man I be really not try to edit war it be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more I can not make any real suggestion on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\ncongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from I as well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense be offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which be contrar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  explanation\\nwhy the edits made under my usern...      0   \n",
       "1  d'aww! he matches this background colour i'm s...      0   \n",
       "2  hey man, i'm really not trying to edit war. it...      0   \n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4  you, sir, are my hero. any chance you remember...      0   \n",
       "5  \"\\n\\ncongratulations from me as well, use the ...      0   \n",
       "6       cocksucker before you piss around on my work      1   \n",
       "7  your vandalism to the matt shirvington article...      0   \n",
       "8  sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  alignment on this subject and which are contra...      0   \n",
       "\n",
       "                                          lemma_text  \n",
       "0  explanation why the edit make under my usernam...  \n",
       "1  d aww he match this background colour I be see...  \n",
       "2  hey man I be really not try to edit war it be ...  \n",
       "3  more I can not make any real suggestion on imp...  \n",
       "4  you sir be my hero any chance you remember wha...  \n",
       "5  congratulation from I as well use the tool wel...  \n",
       "6       cocksucker before you piss around on my work  \n",
       "7  your vandalism to the matt shirvington article...  \n",
       "8  sorry if the word nonsense be offensive to you...  \n",
       "9  alignment on this subject and which be contrar...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].values\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "l = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatize_text(phrase):\n",
    "    phrase_l = l(phrase)\n",
    "    lemma_phrase = \" \".join([token.lemma_ for token in phrase_l])\n",
    "    finished_text = re.sub(r'[^a-zA-Z]', ' ', lemma_phrase) \n",
    "    return \" \".join(finished_text.split())\n",
    "\n",
    "data['lemma_text'] = data['text'].progress_apply(lemmatize_text)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7033360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train shape is (127433, 133938)\n",
      "features_valid shape is (15930, 133938)\n",
      "features_test shape is (15929, 133938)\n"
     ]
    }
   ],
   "source": [
    "features = data['lemma_text']\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "features_test, features_valid, target_test, target_valid = train_test_split(features_test, \n",
    "                                                                              target_test, \n",
    "                                                                              test_size=0.5, \n",
    "                                                                              random_state=12345)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "features_train = count_tf_idf.fit_transform(features_train.values)\n",
    "features_valid = count_tf_idf.transform(features_valid.values)\n",
    "features_test = count_tf_idf.transform(features_test.values)\n",
    "\n",
    "print(f'features_train shape is {features_train.shape}')\n",
    "print(f'features_valid shape is {features_valid.shape}')\n",
    "print(f'features_test shape is {features_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48164425",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300f8bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 метрика на valid данных: 0.7909527073337903\n"
     ]
    }
   ],
   "source": [
    "# Логистическая регрессия\n",
    "\n",
    "model_lr =LogisticRegression(C=10, solver='lbfgs', max_iter=10000, class_weight=1).fit(features_train, target_train)\n",
    "predictions_lr = model_lr.predict(features_valid)\n",
    "f1_score_lr = f1_score(target_valid, predictions_lr)\n",
    "print(f'F1 метрика на valid данных: {f1_score_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c028ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 метрика на valid данных: 0.680029695619896\n"
     ]
    }
   ],
   "source": [
    "# Дерево решений\n",
    "model_dtc = DecisionTreeClassifier(max_depth=25).fit(features_train, target_train)\n",
    "predictions = model_dtc.predict(features_valid)\n",
    "f1_score_dct = f1_score(target_valid, predictions)\n",
    "print(f'F1 метрика на valid данных: {f1_score_dct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c4699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 метрика на valid данных: 0.05162064825930372\n"
     ]
    }
   ],
   "source": [
    "# Случайный лес\n",
    "\n",
    "model_rfc = RandomForestClassifier(n_estimators = 60, max_depth=50).fit(features_train, target_train)\n",
    "predictions = model_rfc.predict(features_valid)\n",
    "f1_score_rfc = f1_score(target_valid, predictions)\n",
    "print(f'F1 метрика на valid данных: {f1_score_rfc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoost\n",
    "\n",
    "model_cb = CatBoostClassifier(verbose=False, iterations=250).fit(features_train, target_train)\n",
    "predictions = model_cb.predict(features_valid)\n",
    "f1_score_cb_cv = cross_val_score(model_cb,\n",
    "                                         features_train, \n",
    "                                         target_train, \n",
    "                                         cv=3, \n",
    "                                         scoring='f1').mean()\n",
    "f1_score_cb = f1_score(target_valid, predictions)\n",
    "print('F1 на кросс-валидации', f1_score_cb_cv)\n",
    "print(f'F1 метрика на valid данных: {f1_score_cb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc04717",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = pd.DataFrame(data = [round(f1_score_lr, 2), round(f1_score_dct, 2), round(f1_score_rfc, 2), round(f1_score_cb, 2)],\n",
    "                         index=['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'CatBoostClassifier'],\n",
    "                         columns = ['F1'])\n",
    "f1_scores = f1_scores.sort_values(by='F1', ascending=False)\n",
    "display(f1_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x = f1_scores.index, y = f1_scores['F1'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0e5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_lr = model_lr.predict(features_test)\n",
    "f1_score_lr = f1_score(target_test, predictions_lr)\n",
    "print(f'F1 метрика на valid данных: {f1_score_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим матрицу ошибок случайного леса\n",
    "cm_lr = confusion_matrix(target_test, predictions_lr)\n",
    "cm_lr_data = pd.DataFrame(cm_lr,\n",
    "                           index=['True Not Canceled', 'True Canceled'],\n",
    "                           columns=['False Not Canceled', 'False Canceled'])\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(cm_lr, xticklabels=cm_lr_data.columns, yticklabels=cm_lr_data.index, annot=True, fmt='g', cmap=\"Reds\", annot_kws={\"size\": 20})\n",
    "plt.title(\"Матрица ошибок\", size=20)\n",
    "plt.xlabel('Предсказания', size=20)\n",
    "plt.ylabel('Реальность', size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ecf640",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102c888",
   "metadata": {},
   "source": [
    "Нам удалось обучить 4 модели на подготовленной выборке и проверить их на 20% от всех данных. Самый лучший результат показала логистическая регрессия - F1 мера приблизительно равна 78%, что является вполне приемлемым результатом, который позволит снизить работу администратора системы примерно в 5 раз.\n",
    "\n",
    "Судя по матрице ошибок наша модель научилась отлично предсказывать нулевые классы - не токсичные комментарии, что не скажешь об обратных. Чтобы улучшим метрику и саму модель необходимо дать на вход намного больше отрицательных комментариев, возможно, сгенерировав их саморучно или с помощью чего-либо."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2747,
    "start_time": "2023-03-18T01:15:45.092Z"
   },
   {
    "duration": 216,
    "start_time": "2023-03-18T01:15:51.488Z"
   },
   {
    "duration": 1479,
    "start_time": "2023-03-18T01:16:30.191Z"
   },
   {
    "duration": 9,
    "start_time": "2023-03-18T01:16:31.780Z"
   },
   {
    "duration": 3851,
    "start_time": "2023-03-18T01:19:00.865Z"
   },
   {
    "duration": 2753,
    "start_time": "2023-03-18T01:19:10.760Z"
   },
   {
    "duration": 295,
    "start_time": "2023-03-18T01:20:13.366Z"
   },
   {
    "duration": 564,
    "start_time": "2023-03-18T01:21:37.959Z"
   },
   {
    "duration": 474,
    "start_time": "2023-03-18T01:21:45.889Z"
   },
   {
    "duration": 1640,
    "start_time": "2023-03-18T01:22:02.240Z"
   },
   {
    "duration": 1563,
    "start_time": "2023-03-18T01:22:20.171Z"
   },
   {
    "duration": 206037,
    "start_time": "2023-03-18T01:23:27.690Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-18T01:27:30.700Z"
   },
   {
    "duration": 510857,
    "start_time": "2023-03-18T01:27:47.099Z"
   },
   {
    "duration": 1363076,
    "start_time": "2023-03-18T01:49:53.673Z"
   },
   {
    "duration": 5470,
    "start_time": "2023-03-18T08:17:41.797Z"
   },
   {
    "duration": 3918,
    "start_time": "2023-03-18T08:17:47.269Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-18T08:17:51.189Z"
   },
   {
    "duration": 655,
    "start_time": "2023-03-18T08:17:56.021Z"
   },
   {
    "duration": 1231628,
    "start_time": "2023-03-18T08:17:57.791Z"
   },
   {
    "duration": 6991,
    "start_time": "2023-03-18T08:38:29.421Z"
   },
   {
    "duration": 135999,
    "start_time": "2023-03-18T08:38:36.414Z"
   },
   {
    "duration": 23,
    "start_time": "2023-03-18T08:43:12.420Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-18T08:43:30.305Z"
   },
   {
    "duration": 20,
    "start_time": "2023-03-18T08:43:58.256Z"
   },
   {
    "duration": 20,
    "start_time": "2023-03-18T08:44:12.152Z"
   },
   {
    "duration": 20,
    "start_time": "2023-03-18T08:44:25.477Z"
   },
   {
    "duration": 55,
    "start_time": "2023-03-18T08:44:50.156Z"
   },
   {
    "duration": 45,
    "start_time": "2023-03-18T08:45:18.533Z"
   },
   {
    "duration": 48,
    "start_time": "2023-03-18T08:45:21.898Z"
   },
   {
    "duration": 50,
    "start_time": "2023-03-18T08:45:25.500Z"
   },
   {
    "duration": 44,
    "start_time": "2023-03-18T08:46:07.993Z"
   },
   {
    "duration": 51,
    "start_time": "2023-03-18T08:47:39.189Z"
   },
   {
    "duration": 23,
    "start_time": "2023-03-18T08:47:49.257Z"
   },
   {
    "duration": 6636,
    "start_time": "2023-03-18T08:53:14.817Z"
   },
   {
    "duration": 57,
    "start_time": "2023-03-18T08:53:29.092Z"
   },
   {
    "duration": 109262,
    "start_time": "2023-03-18T08:54:46.116Z"
   },
   {
    "duration": 15214,
    "start_time": "2023-03-18T08:56:35.381Z"
   },
   {
    "duration": 23113,
    "start_time": "2023-03-18T08:56:50.597Z"
   },
   {
    "duration": 379579,
    "start_time": "2023-03-18T08:57:13.712Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-18T09:03:33.293Z"
   },
   {
    "duration": 5000,
    "start_time": "2023-03-19T01:40:43.196Z"
   },
   {
    "duration": 1141,
    "start_time": "2023-03-19T01:40:48.198Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-19T01:40:49.340Z"
   },
   {
    "duration": 525,
    "start_time": "2023-03-19T01:40:49.348Z"
   },
   {
    "duration": 997008,
    "start_time": "2023-03-19T01:40:49.875Z"
   },
   {
    "duration": 6256,
    "start_time": "2023-03-19T01:57:26.885Z"
   },
   {
    "duration": 109315,
    "start_time": "2023-03-19T01:57:33.143Z"
   },
   {
    "duration": 13078,
    "start_time": "2023-03-19T01:59:22.460Z"
   },
   {
    "duration": 17844,
    "start_time": "2023-03-19T01:59:35.540Z"
   },
   {
    "duration": 2042497,
    "start_time": "2023-03-19T01:59:53.386Z"
   },
   {
    "duration": 150,
    "start_time": "2023-03-19T02:33:55.884Z"
   },
   {
    "duration": 8022,
    "start_time": "2023-03-19T12:07:51.078Z"
   },
   {
    "duration": 1458,
    "start_time": "2023-03-19T12:07:59.102Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-19T12:08:00.562Z"
   },
   {
    "duration": 950,
    "start_time": "2023-03-19T12:08:00.572Z"
   },
   {
    "duration": 1293211,
    "start_time": "2023-03-19T12:08:01.524Z"
   },
   {
    "duration": 7980,
    "start_time": "2023-03-19T12:29:34.737Z"
   },
   {
    "duration": 134697,
    "start_time": "2023-03-19T12:29:42.718Z"
   },
   {
    "duration": 17444,
    "start_time": "2023-03-19T12:31:57.417Z"
   },
   {
    "duration": 26159,
    "start_time": "2023-03-19T12:32:14.863Z"
   },
   {
    "duration": 2465689,
    "start_time": "2023-03-19T12:32:41.030Z"
   },
   {
    "duration": 211,
    "start_time": "2023-03-19T13:13:46.721Z"
   },
   {
    "duration": 17,
    "start_time": "2023-03-19T15:01:12.917Z"
   },
   {
    "duration": 318,
    "start_time": "2023-03-19T15:01:16.377Z"
   },
   {
    "duration": 12482,
    "start_time": "2023-03-19T16:29:16.330Z"
   },
   {
    "duration": 3933,
    "start_time": "2023-03-19T16:29:28.815Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-19T16:29:32.751Z"
   },
   {
    "duration": 886,
    "start_time": "2023-03-19T16:29:32.761Z"
   },
   {
    "duration": 1224550,
    "start_time": "2023-03-19T16:29:33.649Z"
   },
   {
    "duration": 7035,
    "start_time": "2023-03-19T16:49:58.201Z"
   },
   {
    "duration": 121151,
    "start_time": "2023-03-19T16:50:05.238Z"
   },
   {
    "duration": 15188,
    "start_time": "2023-03-19T16:52:06.476Z"
   },
   {
    "duration": 24538,
    "start_time": "2023-03-19T16:52:21.667Z"
   },
   {
    "duration": 3168,
    "start_time": "2023-03-19T19:55:02.561Z"
   },
   {
    "duration": 979,
    "start_time": "2023-03-19T19:55:16.910Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
